{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f806c1",
   "metadata": {},
   "source": [
    "# Benchmark Validation — LLaMA 3.3 70B\n",
    "\n",
    "Compare measured TTFT/ITL numbers against the analytical estimators for the 2×H100 FP8 benchmark in `tested_benchmarks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0aa838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from utils.config import get_model_config, get_hardware_config\n",
    "from utils.math_utils import (\n",
    "    prefill_compute_time,\n",
    "    prefill_memory_time,\n",
    "    decode_compute_time,\n",
    "    decode_memory_time,\n",
    ")\n",
    "\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e212bb",
   "metadata": {},
   "source": [
    "## Load configuration and raw measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2b8499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = Path(\"tested_benchmarks/nim_llama33_70b_v1.8.0_2xH100_fp8TP2.csv\")\n",
    "MODEL_NAME = \"llama33_70B\"\n",
    "HARDWARE_LOOKUP = {\n",
    "    \"H100_80G\": \"H100_80GB_FP8_TP2\",\n",
    "}\n",
    "\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "df_raw[\"hardware_key\"] = df_raw[\"hardware\"].map(HARDWARE_LOOKUP)\n",
    "if df_raw[\"hardware_key\"].isna().any():\n",
    "    missing = df_raw[df_raw[\"hardware_key\"].isna()][\"hardware\"].unique()\n",
    "    raise ValueError(f\"Missing HARDWARE_LOOKUP entries for: {missing}\")\n",
    "\n",
    "model_cfg = get_model_config(MODEL_NAME)\n",
    "hardware_cfg = get_hardware_config(df_raw[\"hardware_key\"].iat[0])\n",
    "display(df_raw.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d6188",
   "metadata": {},
   "source": [
    "## Compute analytical estimates\n",
    "\n",
    "For each scenario we compute the prefill (TTFT) and decode (ITL) bottlenecks, keep both compute/memory components, and convert them to milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb37a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_row(row):\n",
    "    S = float(row[\"concurrency\"])\n",
    "    L_prompt = float(row[\"input_tokens\"])\n",
    "    L_decode = L_prompt + float(row[\"output_tokens\"])\n",
    "\n",
    "    prefill_compute = prefill_compute_time(S, L_prompt, model_cfg, hardware_cfg)\n",
    "    prefill_memory = prefill_memory_time(S, L_prompt, model_cfg, hardware_cfg)\n",
    "    # decode_compute = decode_compute_time(S, L_decode, model_cfg, hardware_cfg)\n",
    "    # decode_memory = decode_memory_time(S, L_decode, model_cfg, hardware_cfg)\n",
    "    decode_compute = decode_compute_time(S, L_prompt, model_cfg, hardware_cfg)\n",
    "    decode_memory = decode_memory_time(S, L_prompt, model_cfg, hardware_cfg, running_tokens_cap=3.03e5)\n",
    "\n",
    "    ttft_compute_ms = float(prefill_compute * 1e3)\n",
    "    ttft_memory_ms = float(prefill_memory * 1e3)\n",
    "    itl_compute_ms = float(decode_compute * 1e3)\n",
    "    itl_memory_ms = float(decode_memory * 1e3)\n",
    "\n",
    "    return pd.Series({\n",
    "        \"prefill_compute_ms\": ttft_compute_ms,\n",
    "        \"prefill_memory_ms\": ttft_memory_ms,\n",
    "        \"ttft_model_ms\": max(ttft_compute_ms, ttft_memory_ms),\n",
    "        \"decode_compute_ms\": itl_compute_ms,\n",
    "        \"decode_memory_ms\": itl_memory_ms,\n",
    "        \"itl_model_ms\": max(itl_compute_ms, itl_memory_ms),\n",
    "    })\n",
    "\n",
    "df_eval = df_raw.join(df_raw.apply(estimate_row, axis=1))\n",
    "df_eval[\"ttft_ratio\"] = df_eval[\"ttft_model_ms\"] / df_eval[\"TTFT_ms\"]\n",
    "df_eval[\"itl_ratio\"] = df_eval[\"itl_model_ms\"] / df_eval[\"ITL_ms\"]\n",
    "df_eval[\"ttft_delta_ms\"] = df_eval[\"ttft_model_ms\"] - df_eval[\"TTFT_ms\"]\n",
    "df_eval[\"itl_delta_ms\"] = df_eval[\"itl_model_ms\"] - df_eval[\"ITL_ms\"]\n",
    "\n",
    "# df_eval = df_eval.sort_values([\"input_tokens\", \"concurrency\"]).reset_index(drop=True)\n",
    "display(df_eval[[\n",
    "    \"input_tokens\", \"concurrency\", \"TTFT_ms\", \"ttft_model_ms\", \"ttft_delta_ms\",\n",
    "    \"ITL_ms\", \"itl_model_ms\", \"itl_delta_ms\",\n",
    "]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca2e438",
   "metadata": {},
   "source": [
    "## Error summary by prompt length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary = (\n",
    "    df_eval\n",
    "    .groupby(\"input_tokens\")[[\n",
    "        \"TTFT_ms\", \"ttft_model_ms\", \"ttft_ratio\",\n",
    "        \"ITL_ms\", \"itl_model_ms\", \"itl_ratio\"\n",
    "    ]]\n",
    "    .agg([\"mean\", \"median\"])\n",
    ")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e728e04",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "The helpers below make it easy to compare measured vs. modelled values for any prompt length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ecb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_metric_vs_concurrency(metric_name, observed_col, model_col, prompt_tokens):\n",
    "    subset = df_eval[df_eval[\"input_tokens\"] == prompt_tokens].sort_values(\"concurrency\")\n",
    "    if subset.empty:\n",
    "        raise ValueError(f\"No rows with input_tokens={prompt_tokens}\")\n",
    "    melted = subset.melt(\n",
    "        id_vars=[\"concurrency\"],\n",
    "        value_vars=[observed_col, model_col],\n",
    "        var_name=\"series\",\n",
    "        value_name=\"value\",\n",
    "    )\n",
    "\n",
    "    title = f\"{metric_name} vs concurrency (prompt={prompt_tokens})\"\n",
    "    fig = px.line(melted, x=\"concurrency\", y=\"value\", color=\"series\", markers=True, title=title)\n",
    "    fig.update_layout(xaxis_title=\"Concurrency\", yaxis_title=f\"{metric_name} [ms]\")\n",
    "    fig.show()\n",
    "\n",
    "df_eval.to_csv(\"eval_results.csv\")\n",
    "\n",
    "for prompt_tokens in sorted(df_eval[\"input_tokens\"].unique()):\n",
    "    plot_metric_vs_concurrency(\"TTFT_ms\", \"TTFT_ms\", \"ttft_model_ms\", prompt_tokens)\n",
    "\n",
    "for prompt_tokens in sorted(df_eval[\"input_tokens\"].unique()):\n",
    "    plot_metric_vs_concurrency(\"ITL_ms\", \"ITL_ms\", \"itl_model_ms\", prompt_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be913832",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig_ttft = px.scatter(\n",
    "    df_eval,\n",
    "    x=\"TTFT_ms\",\n",
    "    y=\"ttft_model_ms\",\n",
    "    color=\"input_tokens\",\n",
    "    hover_data=[\"concurrency\", \"prefill_limit\"],\n",
    "    title=\"TTFT: measured vs modelled\",\n",
    ")\n",
    "fig_ttft.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[df_eval[\"TTFT_ms\"].min(), df_eval[\"TTFT_ms\"].max()],\n",
    "        y=[df_eval[\"TTFT_ms\"].min(), df_eval[\"TTFT_ms\"].max()],\n",
    "        mode=\"lines\",\n",
    "        name=\"ideal\",\n",
    "    )\n",
    ")\n",
    "fig_ttft.update_layout(xaxis_title=\"Measured TTFT [ms]\", yaxis_title=\"Model TTFT [ms]\")\n",
    "fig_ttft.show()\n",
    "\n",
    "fig_itl = px.scatter(\n",
    "    df_eval,\n",
    "    x=\"ITL_ms\",\n",
    "    y=\"itl_model_ms\",\n",
    "    color=\"input_tokens\",\n",
    "    hover_data=[\"concurrency\", \"decode_limit\"],\n",
    "    title=\"ITL: measured vs modelled\",\n",
    ")\n",
    "fig_itl.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[df_eval[\"ITL_ms\"].min(), df_eval[\"ITL_ms\"].max()],\n",
    "        y=[df_eval[\"ITL_ms\"].min(), df_eval[\"ITL_ms\"].max()],\n",
    "        mode=\"lines\",\n",
    "        name=\"ideal\",\n",
    "    )\n",
    ")\n",
    "fig_itl.update_layout(xaxis_title=\"Measured ITL [ms]\", yaxis_title=\"Model ITL [ms]\")\n",
    "fig_itl.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bcf6c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Adjust `MODEL_NAME`, `HARDWARE_LOOKUP`, or the plotting helpers above to explore additional benchmark files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
