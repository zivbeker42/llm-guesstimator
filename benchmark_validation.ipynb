{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac55ed2",
   "metadata": {},
   "source": [
    "\n",
    "# Benchmark Validation\n",
    "\n",
    "Select a benchmark scenario via `SELECTED_SCENARIO` to compare measured TTFT/ITL values with the analytical estimators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from utils.config import get_model_config, get_hardware_config\n",
    "from utils.math_utils import (\n",
    "    prefill_compute_time,\n",
    "    prefill_memory_time,\n",
    "    decode_compute_time,\n",
    "    decode_memory_time,\n",
    ")\n",
    "\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 20\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvalModelBenchmarkConfiguration:\n",
    "    data_path: Path\n",
    "    model_name: str\n",
    "    csv_hardware_label: str\n",
    "    hardware_key: str\n",
    "    running_tokens_cap: float\n",
    "    prefill_running_tokens_cap: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb583c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EVAL_MODEL_BENCHMARK_CONFIGURATION = {\n",
    "    \"llama33_70b_fp8_tp2\": EvalModelBenchmarkConfiguration(\n",
    "        data_path=Path(\"tested_benchmarks/nim_llama33_70b_v1.8.0_2xH100_fp8TP2.csv\"),\n",
    "        model_name=\"llama33_70B\",\n",
    "        csv_hardware_label=\"H100_80G\",\n",
    "        hardware_key=\"H100_80GB_FP8_TP2\",\n",
    "        running_tokens_cap=3.03e5,\n",
    "        prefill_running_tokens_cap=6.25e4\n",
    "    ),\n",
    "    \"llama31_8b_fp8_tp1\": EvalModelBenchmarkConfiguration(\n",
    "        data_path=Path(\"tested_benchmarks/nim_llama3.1_8b_v1.8.0_1xH100_80GB.csv\"),\n",
    "        model_name=\"llama31_8B\",\n",
    "        csv_hardware_label=\"H100_80G\",\n",
    "        hardware_key=\"H100_80GB_FP8_TP1\",\n",
    "        running_tokens_cap=7.5e5,\n",
    "        prefill_running_tokens_cap= 8e4\n",
    "    ),\n",
    "}\n",
    "\n",
    "SELECTED_SCENARIO = \"llama33_70b_fp8_tp2\"\n",
    "CONFIG = EVAL_MODEL_BENCHMARK_CONFIGURATION[SELECTED_SCENARIO]\n",
    "print(f\"Scenario: {SELECTED_SCENARIO}\")\n",
    "print(f\"Running tokens cap: {CONFIG.running_tokens_cap:,.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a9964",
   "metadata": {},
   "source": [
    "## Load configuration and raw measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f70292",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_raw = pd.read_csv(CONFIG.data_path)\n",
    "\n",
    "hardware_values = df_raw[\"hardware\"].unique()\n",
    "if len(hardware_values) != 1 or hardware_values[0] != CONFIG.csv_hardware_label:\n",
    "    raise ValueError(\n",
    "        f\"Unexpected hardware labels {hardware_values}. Expected '{CONFIG.csv_hardware_label}'.\"\n",
    "    )\n",
    "\n",
    "df_raw[\"hardware_key\"] = CONFIG.hardware_key\n",
    "model_cfg = get_model_config(CONFIG.model_name)\n",
    "hardware_cfg = get_hardware_config(CONFIG.hardware_key)\n",
    "\n",
    "print(f\"Loaded rows: {len(df_raw)}\")\n",
    "display(df_raw.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840c9ce5",
   "metadata": {},
   "source": [
    "\n",
    "## Compute analytical estimates\n",
    "\n",
    "For each scenario we compute the prefill (TTFT) and decode (ITL) bottlenecks, keep both compute/memory components, and convert them to milliseconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca72d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_term_pow = 1\n",
    "def estimate_row(row):\n",
    "    S = float(row[\"concurrency\"])\n",
    "    L_prompt = float(row[\"input_tokens\"])\n",
    "    L_decode = L_prompt + float(row[\"output_tokens\"])\n",
    "\n",
    "    prefill_compute = prefill_compute_time(S, L_prompt, model_cfg, hardware_cfg, running_tokens_cap=CONFIG.prefill_running_tokens_cap, prefill_mult_cost=1, phi_term_pow=phi_term_pow )\n",
    "    prefill_memory = prefill_memory_time(S, L_prompt, model_cfg, hardware_cfg)\n",
    "    decode_compute = decode_compute_time(S, L_decode, model_cfg, hardware_cfg)\n",
    "    decode_memory = decode_memory_time(S, L_decode, model_cfg, hardware_cfg, CONFIG.running_tokens_cap)\n",
    "\n",
    "    ttft_compute_ms = float(prefill_compute * 1e3)\n",
    "    ttft_memory_ms = float(prefill_memory * 1e3)\n",
    "    itl_compute_ms = float(decode_compute * 1e3)\n",
    "    itl_memory_ms = float(decode_memory * 1e3)\n",
    "\n",
    "    running_tokens = S * L_decode\n",
    "\n",
    "    return pd.Series({\n",
    "        \"prefill_compute_ms\": ttft_compute_ms,\n",
    "        \"prefill_memory_ms\": ttft_memory_ms,\n",
    "        \"ttft_model_ms\": max(ttft_compute_ms, ttft_memory_ms),\n",
    "        \"prefill_limit\": \"compute\" if ttft_compute_ms >= ttft_memory_ms else \"memory\",\n",
    "        \"decode_compute_ms\": itl_compute_ms,\n",
    "        \"decode_memory_ms\": itl_memory_ms,\n",
    "        \"itl_model_ms\": max(itl_compute_ms, itl_memory_ms),\n",
    "        \"decode_limit\": \"compute\" if itl_compute_ms >= itl_memory_ms else \"memory\",\n",
    "        \"running_tokens\": min(running_tokens, CONFIG.running_tokens_cap),\n",
    "        # \"running_tokens_utilization\": running_tokens / CONFIG.running_tokens_cap,\n",
    "    })\n",
    "\n",
    "\n",
    "df_eval = df_raw.join(df_raw.apply(estimate_row, axis=1))\n",
    "df_eval[\"ttft_ratio\"] = df_eval[\"ttft_model_ms\"] / df_eval[\"TTFT_ms\"]\n",
    "df_eval[\"itl_ratio\"] = df_eval[\"itl_model_ms\"] / df_eval[\"ITL_ms\"]\n",
    "df_eval[\"ttft_delta_ms\"] = df_eval[\"ttft_model_ms\"] - df_eval[\"TTFT_ms\"]\n",
    "df_eval[\"itl_delta_ms\"] = df_eval[\"itl_model_ms\"] - df_eval[\"ITL_ms\"]\n",
    "\n",
    "df_eval = df_eval.sort_values([\"input_tokens\", \"concurrency\"]).reset_index(drop=True)\n",
    "display(df_eval[[\n",
    "    \"input_tokens\", \"concurrency\", \"running_tokens\",\n",
    "    \"TTFT_ms\", \"ttft_model_ms\", \"ttft_delta_ms\",\n",
    "    \"ITL_ms\", \"itl_model_ms\", \"itl_delta_ms\", \"prefill_limit\", \"decode_limit\"\n",
    "]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a448bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "eval_folder = \"evaluations_results\"\n",
    "df_eval.to_csv(f\"{eval_folder}/eval_benchmark-{SELECTED_SCENARIO}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d620374",
   "metadata": {},
   "source": [
    "## Error summary by prompt length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary = (\n",
    "    df_eval\n",
    "    .groupby(\"input_tokens\")[[\n",
    "        \"TTFT_ms\", \"ttft_model_ms\", \"ttft_ratio\",\n",
    "        \"ITL_ms\", \"itl_model_ms\", \"itl_ratio\"\n",
    "    ]]\n",
    "    .agg([\"mean\", \"median\"])\n",
    ")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9779b650",
   "metadata": {},
   "source": [
    "\n",
    "## Plots\n",
    "\n",
    "The helpers below make it easy to compare measured vs. modelled values for any prompt length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0474e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_metric_vs_concurrency(metric_name, observed_col, model_col, prompt_tokens):\n",
    "    subset = df_eval[df_eval[\"input_tokens\"] == prompt_tokens].sort_values(\"concurrency\")\n",
    "    if subset.empty:\n",
    "        raise ValueError(f\"No rows with input_tokens={prompt_tokens}\")\n",
    "    melted = subset.melt(\n",
    "        id_vars=[\"concurrency\"],\n",
    "        value_vars=[observed_col, model_col],\n",
    "        var_name=\"series\",\n",
    "        value_name=\"value\",\n",
    "    )\n",
    "\n",
    "    title = f\"{metric_name} vs concurrency (prompt={prompt_tokens})\"\n",
    "    fig = px.line(melted, x=\"concurrency\", y=\"value\", color=\"series\", markers=True, title=title)\n",
    "    fig.update_layout(xaxis_title=\"Concurrency\", yaxis_title=f\"{metric_name} [ms]\")\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "for prompt_tokens in sorted(df_eval[\"input_tokens\"].unique()):\n",
    "    plot_metric_vs_concurrency(\"TTFT_ms\", \"TTFT_ms\", \"ttft_model_ms\", prompt_tokens)\n",
    "\n",
    "for prompt_tokens in sorted(df_eval[\"input_tokens\"].unique()):\n",
    "    plot_metric_vs_concurrency(\"ITL_ms\", \"ITL_ms\", \"itl_model_ms\", prompt_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ac671",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "context_batches = sorted(df_eval['concurrency'].unique())\n",
    "\n",
    "\n",
    "def plot_metric_vs_context(metric_name, observed_col, model_col, concurrency):\n",
    "    subset = df_eval[df_eval['concurrency'] == concurrency].sort_values('input_tokens')\n",
    "    if subset.empty:\n",
    "        raise ValueError(f'No rows with concurrency={concurrency}')\n",
    "    melted = subset.melt(\n",
    "        id_vars=['input_tokens'],\n",
    "        value_vars=[observed_col, model_col],\n",
    "        var_name='series',\n",
    "        value_name='value',\n",
    "    )\n",
    "    title = f\"{metric_name} vs context length (concurrency={concurrency})\"\n",
    "    fig = px.line(\n",
    "        melted,\n",
    "        x='input_tokens',\n",
    "        y='value',\n",
    "        color='series',\n",
    "        markers=True,\n",
    "        title=title,\n",
    "    )\n",
    "    fig.update_layout(xaxis_title='Prompt tokens', yaxis_title=f'{metric_name} [ms]')\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "for concurrency in context_batches:\n",
    "    plot_metric_vs_context('TTFT_ms', 'TTFT_ms', 'ttft_model_ms', concurrency)\n",
    "\n",
    "for concurrency in context_batches:\n",
    "    plot_metric_vs_context('ITL_ms', 'ITL_ms', 'itl_model_ms', concurrency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697414c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig_ttft = px.scatter(\n",
    "    df_eval,\n",
    "    x=\"TTFT_ms\",\n",
    "    y=\"ttft_model_ms\",\n",
    "    color=\"input_tokens\",\n",
    "    hover_data=[\"concurrency\", \"prefill_limit\", \"running_tokens_utilization\"],\n",
    "    title=\"TTFT: measured vs modelled\",\n",
    ")\n",
    "fig_ttft.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[df_eval[\"TTFT_ms\"].min(), df_eval[\"TTFT_ms\"].max()],\n",
    "        y=[df_eval[\"TTFT_ms\"].min(), df_eval[\"TTFT_ms\"].max()],\n",
    "        mode=\"lines\",\n",
    "        name=\"ideal\",\n",
    "    )\n",
    ")\n",
    "fig_ttft.update_layout(xaxis_title=\"Measured TTFT [ms]\", yaxis_title=\"Model TTFT [ms]\")\n",
    "fig_ttft.show()\n",
    "\n",
    "fig_itl = px.scatter(\n",
    "    df_eval,\n",
    "    x=\"ITL_ms\",\n",
    "    y=\"itl_model_ms\",\n",
    "    color=\"input_tokens\",\n",
    "    hover_data=[\"concurrency\", \"decode_limit\", \"running_tokens_utilization\"],\n",
    "    title=\"ITL: measured vs modelled\",\n",
    ")\n",
    "fig_itl.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[df_eval[\"ITL_ms\"].min(), df_eval[\"ITL_ms\"].max()],\n",
    "        y=[df_eval[\"ITL_ms\"].min(), df_eval[\"ITL_ms\"].max()],\n",
    "        mode=\"lines\",\n",
    "        name=\"ideal\",\n",
    "    )\n",
    ")\n",
    "fig_itl.update_layout(xaxis_title=\"Measured ITL [ms]\", yaxis_title=\"Model ITL [ms]\")\n",
    "fig_itl.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbe1160",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Adjust `SELECTED_SCENARIO` or extend `EVAL_MODEL_BENCHMARK_CONFIGURATION` as additional benchmark files become available.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
