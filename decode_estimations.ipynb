{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319afeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.colab\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = \"https://github.com/zivbeker42/llm-guesstimator.git\"\n",
    "\n",
    "bash_setup = f\"\"\"\n",
    "setup_llm_guesstimator() {{\n",
    "  if [[ -n \\\"${{COLAB_RELEASE_TAG:-}}\\\" ]]; then\n",
    "    local repo_path=\\\"/content/llm-guesstimator\\\"\n",
    "    if [[ ! -d \\\"$repo_path\\\" ]]; then\n",
    "      git clone \\\"{REPO_URL}\\\" \\\"$repo_path\\\"\n",
    "    fi\n",
    "  fi\n",
    "}}\n",
    "setup_llm_guesstimator\n",
    "\"\"\"\n",
    "\n",
    "subprocess.run([\"bash\", \"-lc\", bash_setup])\n",
    "\n",
    "repo_path = \"/content/llm-guesstimator\" if \"google.colab\" in sys.modules else os.path.abspath('.')\n",
    "\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "os.chdir(repo_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ad141",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy matplotlib pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e82a82a",
   "metadata": {
    "id": "6e82a82a"
   },
   "source": [
    "\n",
    "# Transformer Inference Compute Estimates (KV-Cache)\n",
    "\n",
    "This notebook walks from basic matrix-multiplication complexity to practical, closed-form estimates for **per-token** compute and **time** during autoregressive decoding with **KV-cache**. It includes:\n",
    "\n",
    "1. A quick refresher on matrix multiplication FLOPs.\n",
    "2. Per-layer FLOPs for a Transformer block with and without KV-cache.\n",
    "3. A server-level compute model with **fixed token budget** $(P = S\\cdot L)$ (concurrent requests \\(S\\), context length \\(L\\)).\n",
    "4. Plots of total **time per new decoded token** vs. \\(L\\) and \\(S\\) for a given GPU peak FLOPs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95acd85f",
   "metadata": {
    "id": "95acd85f"
   },
   "source": [
    "\n",
    "## 1) Matrix multiplication complexity (short refresher)\n",
    "\n",
    "Multiplying an \\((m \\times n)\\) matrix by an \\((n \\times p)\\) matrix costs approximately:\n",
    "\n",
    "$$\n",
    "\\text{FLOPs} \\approx 2\\,m\\,n\\,p\n",
    "$$\n",
    "\n",
    "(counting a multiply and an add as two FLOPs). This is the core building block behind linear layers in Transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9badbce0",
   "metadata": {
    "id": "9badbce0"
   },
   "source": [
    "\n",
    "## 2) Transformer per-layer FLOPs\n",
    "\n",
    "We separate each Transformer layer into **attention** and **FFN** parts. Let:\n",
    "\n",
    "- \\(L\\): sequence length / context length (number of tokens attended over).\n",
    "- \\(d\\): hidden size.\n",
    "- \\(h\\): number of attention heads (each head size $(d_h = d/h)$).\n",
    "- \\(r\\): FFN expansion ratio (usually $(r \\approx 4$)).\n",
    "\n",
    "### 2.1) Without KV-cache (full recomputation)\n",
    "Processing a length-\\(L\\) sequence **per layer** costs approximately:\n",
    "\n",
    "$$\n",
    "\\underbrace{8Ld^2}_{Q,K,V,O \\text{ projections}} \\;+\\; \\underbrace{4L^2 d}_{QK^\\top \\text{ and weights}\\times V} \\;+\\; \\underbrace{4rLd^2}_{\\text{FFN}}\n",
    "\\;=\\; (8+4r)Ld^2 \\;+\\; 4L^2 d.\n",
    "$$\n",
    "\n",
    "\n",
    "### 2.2) With KV-cache (incremental decode, per **new token**)\n",
    "Only the new token's attention is computed (past keys/values are reused). Per layer, per new token:\n",
    "\n",
    "$$\n",
    "\\underbrace{8d^2}_{Q,K,V,O \\text{ for the new token}} \\;+\\; \\underbrace{4Ld}_{qK^\\top \\text{ and weights}\\times V} \\;+\\; \\underbrace{4rd^2}_{\\text{FFN}}\n",
    "\\;=\\; (8+4r)d^2 \\;+\\; 4Ld.\n",
    "$$\n",
    "\n",
    "\n",
    "We will use this last expression as our **per-request, per-layer, per-token** FLOPs for decoding with KV-cache.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2809f754",
   "metadata": {
    "id": "2809f754"
   },
   "source": [
    "\n",
    "## 3) Server-level model with fixed token budget $(P = S\\cdot L)$\n",
    "\n",
    "Let:\n",
    "- $S$: number of concurrent requests (each decoding one new token at a step).\n",
    "- $P = S \\cdot L$: fixed \"token budget\" across the server at each decoding step.\n",
    "\n",
    "Then total **per-layer** FLOPs across all \\(S\\) requests is:\n",
    "\n",
    "$$\n",
    "\\big((8+4r)d^2 + 4Ld\\big)\\,S\n",
    "\\;=\\;\n",
    "\\frac{P(8+4r)d^2}{L} \\;+\\; 4Pd.\n",
    "$$\n",
    "\n",
    "\n",
    "This exhibits a \\(1/L\\) term plus a constant \\(2Pd\\) asymptote.\n",
    "\n",
    "For the full model with $n_\\text{layers}$ layers, multiply by $n_\\text{layers}$. To convert to **time per token**, divide by **GPU peak FLOPs/sec**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b388a6d",
   "metadata": {
    "id": "8b388a6d"
   },
   "source": [
    "\n",
    "## 4) Parameters\n",
    "\n",
    "We define the following parameters in code:\n",
    "\n",
    "- `r` — FFN expansion ratio (default: 4).\n",
    "- `d` — hidden size (default: 4096).\n",
    "- `P` — fixed token budget $P=S\\cdot L$ (default: 20,000).\n",
    "- `n_layers` — number of Transformer layers (default: 64).\n",
    "- `gpu_flops_per_sec` — GPU peak FLOPs/sec (default: 312e12 for NVIDIA A100 ~312 TFLOPS).\n",
    "\n",
    "You can adjust these to match your setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ff5fe",
   "metadata": {
    "id": "f87ff5fe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from utils.config import (\n",
    "    GRID_SETTINGS,\n",
    "    DEFAULT_DECODE_MODEL_NAME,\n",
    "    get_hardware_config,\n",
    "    get_model_config,\n",
    "    get_workload_config,\n",
    ")\n",
    "from utils.math_utils import (\n",
    "    activation_io_bytes,\n",
    "    decode_memory_time,\n",
    "    kv_read_bytes,\n",
    "    kv_write_bytes,\n",
    "    time_per_token_from_L,\n",
    "    time_per_token_from_S,\n",
    "    weights_bytes,\n",
    ")\n",
    "\n",
    "hardware = get_hardware_config()\n",
    "workload = get_workload_config()\n",
    "decode_model = get_model_config(DEFAULT_DECODE_MODEL_NAME)\n",
    "\n",
    "P = workload.total_prompt_tokens\n",
    "d = decode_model.hidden_size\n",
    "r = decode_model.expansion_ratio\n",
    "n_layers = decode_model.num_layers\n",
    "gpu_flops_per_sec = hardware.flops_per_second\n",
    "\n",
    "decode_settings = GRID_SETTINGS[\"decode\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6034971",
   "metadata": {
    "id": "f6034971"
   },
   "source": [
    "\n",
    "## 5) Functions for FLOPs and time\n",
    "We implement small helpers for clarity and reuse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39976dee",
   "metadata": {
    "id": "39976dee"
   },
   "source": [
    "\n",
    "## 6) Plot: Time per token vs. \\(L\\) (with fixed \\(P\\))\n",
    "\n",
    "This plots $(T(L) = \\dfrac{n_\\text{layers}}{\\text{GPU}}\\left(\\dfrac{P(8+4r)d^2}{L} + 4Pd\\right))$ and the asymptote $(\\dfrac{n_\\text{layers} 4Pd}{\\text{GPU}})$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd4eb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "executionInfo": {
     "elapsed": 1096,
     "status": "ok",
     "timestamp": 1759258805503,
     "user": {
      "displayName": "ziv beker",
      "userId": "07904590467861271362"
     },
     "user_tz": -180
    },
    "id": "21cd4eb4",
    "outputId": "47f9e31f-609e-4c7d-b4b4-1aeb0c2dc7e4"
   },
   "outputs": [],
   "source": [
    "L_vals = np.logspace(\n",
    "    np.log10(decode_settings[\"initial_context_min\"]),\n",
    "    np.log10(P),\n",
    "    int(decode_settings[\"initial_context_samples\"]),\n",
    ")\n",
    "T_L = time_per_token_from_L(L_vals, P, decode_model, hardware)\n",
    "asymptote_time = (4 * P * d * n_layers) / gpu_flops_per_sec\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(L_vals, T_L, label=\"T(L) with P fixed\")\n",
    "plt.axhline(\n",
    "    asymptote_time,\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Asymptote = {asymptote_time * 1e6:.3f} µs\",\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Context length L (log scale)\")\n",
    "plt.ylabel(\"Seconds per new token (across S concurrent reqs)\")\n",
    "plt.title(\"Per-token time vs. L with fixed P\")\n",
    "plt.grid(True, which=\"both\", linestyle=\":\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde4ccf",
   "metadata": {
    "id": "9bde4ccf"
   },
   "source": [
    "\n",
    "## 7) Combined plot: \\(T(L)\\) and \\(T(S)\\) on the same figure\n",
    "\n",
    "- Bottom x-axis: $L$ (log scale).\n",
    "- Top x-axis: $S$ (linear), related by $S = P / L$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc572c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1759258946615,
     "user": {
      "displayName": "ziv beker",
      "userId": "07904590467861271362"
     },
     "user_tz": -180
    },
    "id": "4ebc572c",
    "outputId": "d6bd4e34-26e8-4e66-fb5a-4ee29e4bc69f"
   },
   "outputs": [],
   "source": [
    "start, stop, num = decode_settings[\"concurrency_range\"]\n",
    "S_vals = np.linspace(start, stop, int(num))\n",
    "T_S = time_per_token_from_S(S_vals, P, decode_model, hardware)\n",
    "\n",
    "min_context = max(decode_settings[\"secondary_context_min\"], P / stop)\n",
    "L_vals = np.logspace(\n",
    "    np.log10(min_context),\n",
    "    np.log10(P),\n",
    "    int(decode_settings[\"secondary_context_samples\"]),\n",
    ")\n",
    "T_L = time_per_token_from_L(L_vals, P, decode_model, hardware)\n",
    "\n",
    "fig, axL = plt.subplots(figsize=(8, 5))\n",
    "axL.plot(L_vals, T_L, label=\"T(L) with P fixed\", linewidth=2)\n",
    "axL.set_xscale(\"log\")\n",
    "axL.set_xlabel(\"Context length L (log scale)\")\n",
    "axL.set_ylabel(\"Seconds per new token (across S concurrent reqs)\")\n",
    "axL.grid(True, which=\"both\", linestyle=\":\")\n",
    "\n",
    "axS = axL.twiny()\n",
    "axS.plot(S_vals, T_S, label=\"T(S) with P fixed\", alpha=0.85, color=\"red\")\n",
    "axS.set_xlabel(\"Concurrency S (linear scale)\")\n",
    "\n",
    "lines_L, labels_L = axL.get_legend_handles_labels()\n",
    "lines_S, labels_S = axS.get_legend_handles_labels()\n",
    "axL.legend(lines_L + lines_S, labels_L + labels_S, loc=\"upper right\")\n",
    "\n",
    "plt.title(\"Per-token time with fixed P = S·L\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o42oO47Pts6y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "executionInfo": {
     "elapsed": 1064,
     "status": "ok",
     "timestamp": 1759224650529,
     "user": {
      "displayName": "ziv beker",
      "userId": "07904590467861271362"
     },
     "user_tz": -180
    },
    "id": "o42oO47Pts6y",
    "outputId": "8d805114-aa11-43cb-df90-9af15ae178e1"
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "# S_vals = np.linspace(5000, 1, 600)\n",
    "# T_S = time_per_token_S(S_vals, P, d, r, n_layers, gpu_flops_per_sec)\n",
    "\n",
    "# L_vals = np.logspace(np.log10(max(4, P/5000)), np.log10(P), 600)\n",
    "# T_L = time_per_token_L(L_vals, P, d, r, n_layers, gpu_flops_per_sec)\n",
    "\n",
    "# fig, axL = plt.subplots(figsize=(8,5))\n",
    "\n",
    "# axS = axL.twiny()\n",
    "# axS.plot(S_vals, T_S, label=\"T(S) with P fixed\", alpha=0.85)\n",
    "# axS.set_xlabel(\"Concurrency S (linear scale)\")\n",
    "\n",
    "# axL.plot(L_vals, T_L, label=\"T(L) with P fixed\", linewidth=2)\n",
    "# axL.set_xscale(\"log\")\n",
    "# axL.set_xlabel(\"Context length L (log scale)\")\n",
    "# axL.set_ylabel(\"Seconds per new token (across S concurrent reqs)\")\n",
    "# axL.grid(True, which=\"both\", linestyle=\":\")\n",
    "\n",
    "# lines_L, labels_L = axL.get_legend_handles_labels()\n",
    "# lines_S, labels_S = axS.get_legend_handles_labels()\n",
    "# axL.legend(lines_L + lines_S, labels_L + labels_S, loc=\"upper right\")\n",
    "\n",
    "# plt.title(\"Per-token time with fixed P = S·L\")\n",
    "# plt.tight_layout()\n",
    "# # plt.show()\n",
    "# # S_vals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd1cda",
   "metadata": {
    "id": "b4dd1cda"
   },
   "source": [
    "\n",
    "## 8) Sample values table\n",
    "\n",
    "Representative points for \\(L\\) and \\(S\\) with the current parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd5b83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 739
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1759258978398,
     "user": {
      "displayName": "ziv beker",
      "userId": "07904590467861271362"
     },
     "user_tz": -180
    },
    "id": "84dd5b83",
    "outputId": "99c166e4-7885-4fb2-ca5d-9a314f9e5ff7"
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for L in [50, 100, 200, 500, 1000, 2000, 4000, 8000, 10000, 20000]:\n",
    "    S = P / L\n",
    "    rows.append({\n",
    "        \"L (context)\": L,\n",
    "        \"S (concurrency)\": S,\n",
    "        \"Time per token (s)\": float(time_per_token_from_L(L, P, decode_model, hardware)),\n",
    "        \"Time per token (ms)\": float(time_per_token_from_L(L, P, decode_model, hardware) * 1e3),\n",
    "    })\n",
    "\n",
    "for S in [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000]:\n",
    "    L = P / S\n",
    "    rows.append({\n",
    "        \"S (concurrency)\": S,\n",
    "        \"L (context)\": L,\n",
    "        \"Time per token (s)\": float(time_per_token_from_S(S, P, decode_model, hardware)),\n",
    "        \"Time per token (ms)\": float(time_per_token_from_S(S, P, decode_model, hardware) * 1e3),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3f5b3",
   "metadata": {
    "id": "9dc3f5b3"
   },
   "source": [
    "\n",
    "# Memory-Bound Estimates for Transformer Decoding (KV-Cache)\n",
    "\n",
    "This companion notebook focuses **only on memory traffic** for per-token latency estimates during autoregressive decoding with KV-cache.\n",
    "It is designed to be appended to the previous compute-focused notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de32213",
   "metadata": {
    "id": "3de32213"
   },
   "source": [
    "\n",
    "## Parameters\n",
    "\n",
    "- $r$ — FFN expansion ratio (default: 4)  \n",
    "- $d$ — hidden size (default: 4096)  \n",
    "- $P$ — fixed token budget $P=S\\cdot L$ (default: 20,000)  \n",
    "- $n_\\text{layers}$ — number of Transformer layers (default: 64)  \n",
    "- `dtype_bytes` — bytes per element (default: 2 for FP16/BF16)  \n",
    "- `BW` — memory bandwidth in Bytes/sec (e.g., A100-80GB $\\approx 2.0\\times 10^{12}$, A100-40GB $\\approx 1.555\\times 10^{12}$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410bf11",
   "metadata": {
    "id": "5410bf11"
   },
   "source": [
    "\n",
    "## Plots\n",
    "\n",
    "- $T_{\\text{memory}}(L)$ with fixed $P=S\\cdot L$ (log-$L$ axis)  \n",
    "- $T_{\\text{memory}}(S)$ with fixed $P$ (linear-$S$ axis)  \n",
    "\n",
    "The asymptotic floor at large $L$ (small $S$) is given by **weights + KV read** only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05892ad",
   "metadata": {},
   "source": [
    "## Memory Traffic Model\n",
    "\n",
    "We approximate the per-token memory bytes (across all $S$ concurrent requests at a step) as the sum of:\n",
    "\n",
    "1. **Weights read (dominant):** $(4+2r)d^2$ params per layer  \n",
    "   Bytes: $n_\\text{layers}\\,(4+2r)\\,d^2 \\cdot \\text{dtype\\_bytes}$\n",
    "\n",
    "2. **KV-cache read (constant vs $L$ at fixed $P$):** across $S$ requests per layer we read both $K$ and $V$ for past $L$ tokens  \n",
    "   Elements: $2SLd = 2Pd$  \n",
    "   Bytes: $n_\\text{layers}\\,2Pd \\cdot \\text{dtype\\_bytes}$\n",
    "\n",
    "3. **KV-cache write (grows with $S$):** writing new $K,V$ for the new token per request  \n",
    "   Elements: $2Sd$  \n",
    "   Bytes: $n_\\text{layers}\\,2Sd \\cdot \\text{dtype\\_bytes}$\n",
    "\n",
    "4. **Activation I/O (smaller term, but included):** for the six linears (Q,K,V,O and 2xFFN) we use a factor $c_\\text{act}=12$  \n",
    "   Bytes: $n_\\text{layers}\\,c_\\text{act}\\,Sd \\cdot \\text{dtype\\_bytes}$\n",
    "\n",
    "The **memory-bound time** is  \n",
    "$T_\\text{memory} = \\dfrac{\\text{Bytes}}{\\text{total\\_BW}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a2a5c",
   "metadata": {},
   "source": [
    "\n",
    "## Plots\n",
    "\n",
    "- $T_{\\text{memory}}(L)$ with fixed $P=S\\cdot L$ (log-$L$ axis)  \n",
    "- $T_{\\text{memory}}(S)$ with fixed $P$ (linear-$S$ axis)  \n",
    "\n",
    "The asymptotic floor at large $L$ (small $S$) is given by **weights + KV read** only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbed3089",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "executionInfo": {
     "elapsed": 629,
     "status": "ok",
     "timestamp": 1759259169960,
     "user": {
      "displayName": "ziv beker",
      "userId": "07904590467861271362"
     },
     "user_tz": -180
    },
    "id": "cbed3089",
    "outputId": "182eed3f-768e-4eae-f6a7-faa6838d59b2"
   },
   "outputs": [],
   "source": [
    "start, stop, num = decode_settings[\"concurrency_range\"]\n",
    "S_vals = np.linspace(start, stop, int(num))\n",
    "L_for_S = P / S_vals\n",
    "Tmem_S = decode_memory_time(S_vals, L_for_S, decode_model, hardware)\n",
    "\n",
    "min_context = max(decode_settings[\"secondary_context_min\"], P / stop)\n",
    "L_vals = np.logspace(\n",
    "    np.log10(min_context),\n",
    "    np.log10(P),\n",
    "    int(decode_settings[\"secondary_context_samples\"]),\n",
    ")\n",
    "S_for_L = P / L_vals\n",
    "Tmem_L = decode_memory_time(S_for_L, L_vals, decode_model, hardware)\n",
    "\n",
    "T_asym = float(decode_memory_time(1.0, P, decode_model, hardware))\n",
    "\n",
    "fig, axL = plt.subplots(figsize=(8, 5))\n",
    "axL.plot(L_vals, Tmem_L * 1e3, label=\"T_memory(L) (ms)\", linewidth=2)\n",
    "axL.axhline(T_asym * 1e3, linestyle=\"--\", label=f\"Asymptote ≈ {T_asym * 1e3:.2f} ms\")\n",
    "axL.set_xscale(\"log\")\n",
    "axL.set_xlabel(\"Context length L (log scale)\")\n",
    "axL.set_ylabel(\"Estimated memory time per token (ms)\")\n",
    "axL.grid(True, which=\"both\", linestyle=\":\")\n",
    "\n",
    "axS = axL.twiny()\n",
    "axS.plot(S_vals, Tmem_S * 1e3, label=\"T_memory(S) (ms)\", alpha=0.85, color=\"red\")\n",
    "axS.set_xlabel(\"Concurrency S (linear scale)\")\n",
    "\n",
    "lines_L, labels_L = axL.get_legend_handles_labels()\n",
    "lines_S, labels_S = axS.get_legend_handles_labels()\n",
    "axL.legend(lines_L + lines_S, labels_L + labels_S, loc=\"upper right\")\n",
    "\n",
    "plt.title(\"Memory-bound per-token time (pick BW as needed)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa880f",
   "metadata": {
    "id": "7baa880f"
   },
   "source": [
    "\n",
    "## Sample Points\n",
    "\n",
    "A small table of representative $(L,S)$ values to validate magnitudes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66d8fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1759259373188,
     "user": {
      "displayName": "ziv beker",
      "userId": "07904590467861271362"
     },
     "user_tz": -180
    },
    "id": "0a66d8fc",
    "outputId": "038dd586-cdb7-4330-d03d-383de65cafe4"
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for L in [50, 200, 1000, 4000, 20000]:\n",
    "    S = P / L\n",
    "    rows.append({\n",
    "        \"L\": L,\n",
    "        \"S\": S,\n",
    "        \"Bytes_weights_GB\": weights_bytes(decode_model, hardware) / 1e9,\n",
    "        \"Bytes_KV_read_GB\": kv_read_bytes(P, decode_model, hardware) / 1e9,\n",
    "        \"Bytes_KV_write_GB\": kv_write_bytes(S, decode_model, hardware) / 1e9,\n",
    "        \"Bytes_act_IO_GB\": activation_io_bytes(S, decode_model, hardware) / 1e9,\n",
    "        \"T_mem_ms\": float(decode_memory_time(S, L, decode_model, hardware) * 1e3),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0VI_yRMWkacz",
   "metadata": {
    "id": "0VI_yRMWkacz"
   },
   "source": [
    "#combine memory and bandwidth time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q6jbngMUaRdx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15447,
     "status": "ok",
     "timestamp": 1759232229750,
     "user": {
      "displayName": "ziv beker",
      "userId": "07904590467861271362"
     },
     "user_tz": -180
    },
    "id": "q6jbngMUaRdx",
    "outputId": "dd5fe382-ce94-497e-d912-fe7fd9c9941d"
   },
   "outputs": [],
   "source": [
    "# Install plotly if needed\n",
    "!pip install plotly nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75NRTRD5aOw0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2895,
     "status": "ok",
     "timestamp": 1759259381303,
     "user": {
      "displayName": "ziv beker",
      "userId": "07904590467861271362"
     },
     "user_tz": -180
    },
    "id": "75NRTRD5aOw0",
    "outputId": "0b66387a-af44-4207-fce5-508585d2bb35"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from utils.config import decode_surface_grids, prefill_surface_grids, get_model_config\n",
    "from utils.math_utils import (\n",
    "    decode_compute_time,\n",
    "    decode_memory_time,\n",
    "    prefill_compute_time,\n",
    "    prefill_memory_time,\n",
    "    safe_ratio,\n",
    ")\n",
    "\n",
    "prefill_model = get_model_config(\"70B\")\n",
    "decode_surface_S, decode_surface_L = decode_surface_grids()\n",
    "prefill_surface_S, prefill_surface_L = prefill_surface_grids()\n",
    "\n",
    "prefill_S_vals = np.array(prefill_surface_S, dtype=float)\n",
    "prefill_L_vals = np.arange(\n",
    "    prefill_surface_L[0], prefill_surface_L[1] + prefill_surface_L[2], prefill_surface_L[2], dtype=float\n",
    ")\n",
    "\n",
    "prefill_Sg = prefill_S_vals[None, :]\n",
    "prefill_Lg = prefill_L_vals[:, None]\n",
    "\n",
    "prefill_T_compute = prefill_compute_time(prefill_Sg, prefill_Lg, prefill_model, hardware)\n",
    "prefill_T_memory = prefill_memory_time(prefill_Sg, prefill_Lg, prefill_model, hardware)\n",
    "prefill_T_ratio = safe_ratio(prefill_T_compute, prefill_T_memory)\n",
    "prefill_T_max = np.maximum(prefill_T_compute, prefill_T_memory)\n",
    "\n",
    "prefill_eps = 0.05\n",
    "prefill_den = np.maximum(prefill_T_compute, prefill_T_memory)\n",
    "prefill_boundary = np.abs(prefill_T_compute - prefill_T_memory) / np.where(prefill_den == 0, 1, prefill_den) <= prefill_eps\n",
    "pi, pj = np.where(prefill_boundary)\n",
    "prefill_boundary_L = prefill_L_vals[pi]\n",
    "prefill_boundary_S = prefill_S_vals[pj]\n",
    "prefill_boundary_Z = prefill_T_compute[pi, pj]\n",
    "\n",
    "def make_surface(x_vals, y_vals, z, boundary_S, boundary_L, boundary_Z, title, zlabel):\n",
    "    fig = go.Figure(\n",
    "        data=[\n",
    "            go.Surface(x=x_vals, y=y_vals, z=z, showscale=True),\n",
    "            go.Scatter3d(\n",
    "                x=boundary_S,\n",
    "                y=boundary_L,\n",
    "                z=boundary_Z,\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=3, opacity=0.8),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis_title=\"S (batch size)\",\n",
    "            yaxis_title=\"L (tokens)\",\n",
    "            zaxis_title=zlabel,\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, t=40, b=0),\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "make_surface(\n",
    "    prefill_S_vals,\n",
    "    prefill_L_vals,\n",
    "    prefill_T_compute,\n",
    "    prefill_boundary_S,\n",
    "    prefill_boundary_L,\n",
    "    prefill_boundary_Z,\n",
    "    \"Prefill — T_compute(S,L)\",\n",
    "    \"Time per prefill step (s)\",\n",
    ")\n",
    "\n",
    "make_surface(\n",
    "    prefill_S_vals,\n",
    "    prefill_L_vals,\n",
    "    prefill_T_memory,\n",
    "    prefill_boundary_S,\n",
    "    prefill_boundary_L,\n",
    "    prefill_boundary_Z,\n",
    "    \"Prefill — T_memory(S,L)\",\n",
    "    \"Time per prefill step (s)\",\n",
    ")\n",
    "\n",
    "make_surface(\n",
    "    prefill_S_vals,\n",
    "    prefill_L_vals,\n",
    "    prefill_T_ratio,\n",
    "    prefill_boundary_S,\n",
    "    prefill_boundary_L,\n",
    "    prefill_boundary_Z,\n",
    "    \"Prefill — T_compute/T_memory(S,L)\",\n",
    "    \"Ratio\",\n",
    ")\n",
    "\n",
    "decode_S_start, decode_S_stop, decode_S_step = decode_surface_S\n",
    "decode_L_start, decode_L_stop, decode_L_step = decode_surface_L\n",
    "decode_S_vals = np.arange(decode_S_start, decode_S_stop, decode_S_step, dtype=float)\n",
    "decode_L_vals = np.arange(\n",
    "    decode_L_start,\n",
    "    decode_L_stop + decode_L_step,\n",
    "    decode_L_step,\n",
    "    dtype=float,\n",
    ")\n",
    "\n",
    "decode_Sg = decode_S_vals[None, :]\n",
    "decode_Lg = decode_L_vals[:, None]\n",
    "\n",
    "decode_T_compute = decode_compute_time(decode_Sg, decode_Lg, prefill_model, hardware)\n",
    "decode_T_memory = decode_memory_time(decode_Sg, decode_Lg, prefill_model, hardware)\n",
    "decode_T_max = np.maximum(decode_T_compute, decode_T_memory)\n",
    "\n",
    "decode_eps = 0.05\n",
    "decode_den = np.maximum(decode_T_compute, decode_T_memory)\n",
    "decode_boundary = np.abs(decode_T_compute - decode_T_memory) / np.where(decode_den == 0, 1, decode_den) <= decode_eps\n",
    "di, dj = np.where(decode_boundary)\n",
    "decode_boundary_L = decode_L_vals[di]\n",
    "decode_boundary_S = decode_S_vals[dj]\n",
    "decode_boundary_Z = decode_T_compute[di, dj]\n",
    "\n",
    "make_surface(\n",
    "    decode_S_vals,\n",
    "    decode_L_vals,\n",
    "    decode_T_compute,\n",
    "    decode_boundary_S,\n",
    "    decode_boundary_L,\n",
    "    decode_boundary_Z,\n",
    "    \"Decode — T_compute(S,L) per token\",\n",
    "    \"Time per token (s)\",\n",
    ")\n",
    "\n",
    "make_surface(\n",
    "    decode_S_vals,\n",
    "    decode_L_vals,\n",
    "    decode_T_memory,\n",
    "    decode_boundary_S,\n",
    "    decode_boundary_L,\n",
    "    decode_boundary_Z,\n",
    "    \"Decode — T_memory(S,L) per token\",\n",
    "    \"Time per token (s)\",\n",
    ")\n",
    "\n",
    "make_surface(\n",
    "    decode_S_vals,\n",
    "    decode_L_vals,\n",
    "    decode_T_max,\n",
    "    decode_boundary_S,\n",
    "    decode_boundary_L,\n",
    "    decode_boundary_Z,\n",
    "    \"Decode — max(T) per token\",\n",
    "    \"Time per token (s)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fty5gevfaYey",
   "metadata": {
    "id": "fty5gevfaYey"
   },
   "outputs": [],
   "source": [
    "# citing from flash2.pdf\n",
    "\n",
    "Benchmark setting: we vary the sequence length from 512, 1k, ..., 16k, and set batch size so that the total\n",
    "number of tokens is 16k. We set hidden dimension to 2048, and head dimension to be either 64 or 128 (i.e.,\n",
    "32 heads or 16 heads). To calculate the FLOPs of the forward pass, we use:\n",
    "4 · seqlen2 · head dimension · number of heads."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
